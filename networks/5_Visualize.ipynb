{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Visualization\n",
    "In this part of the tutorial, you will learn how to visualize the networks and embeddings produced so far. We will use projection techniques and helios-web\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import emlens\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "import xnetwork as xn\n",
    "import igraph as ig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName = \"debate2024_Jun_bluesky\"\n",
    "dataPath = Path(\"Data\")\n",
    "networksPath = dataPath/\"Networks\"\n",
    "networksPath.mkdir(parents=True, exist_ok=True)\n",
    "bertModelName = \"all-MiniLM-L12-v2\"\n",
    "# bertModelName = \"all-mpnet-base-v2\"\n",
    "# bertModelName = \"dmlls/all-mpnet-base-v2-negation\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(dataPath / f\"{datasetName}.feather.gz\", 'rb') as f:\n",
    "    df = pd.read_feather(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to load embeddings\n",
    "with np.load(dataPath / f\"{datasetName}_{bertModelName.replace('/','_')}_embeddings.npz\") as data:\n",
    "    sentence_embeddings = data[\"sentence_embeddings\"]\n",
    "    uniqueSentences = data[\"uniqueSentences\"]\n",
    "    sentenceIndices = data[\"sentenceIndices\"]\n",
    "    modelName = str(data[\"modelName\"])\n",
    "    \n",
    "model = SentenceTransformer(modelName)\n",
    "\n",
    "# helper function\n",
    "def emb(sentence):\n",
    "    return model.encode(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a network based on the nearest neighbors of each post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "numberNeighbors = 3\n",
    "nnModel = NearestNeighbors(n_neighbors=numberNeighbors, metric='cosine').fit(sentence_embeddings)\n",
    "edges = []\n",
    "weights = []\n",
    "chunkSize = 1000\n",
    "for chunkIndex in tqdm(range(0,len(sentence_embeddings)+1,chunkSize)):\n",
    "    chunk = slice(chunkIndex, min(chunkIndex+chunkSize, len(sentence_embeddings)))\n",
    "    distances, indices = nnModel.kneighbors(sentence_embeddings[chunk])\n",
    "    for i, (distances, indices) in enumerate(zip(distances, indices)):\n",
    "        for distance, index in zip(distances, indices):\n",
    "            if distance < 0.5:\n",
    "                edges.append((chunk.start+i, index))\n",
    "                weights.append(1-distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper dictionaries in case we need to convert from the sentence indices to the original indices in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceIndex2DataIndices = []\n",
    "for i in range(len(uniqueSentences)):\n",
    "    sentenceIndex2DataIndices.append(set())\n",
    "for i, sentenceIndex in enumerate(sentenceIndices):\n",
    "    sentenceIndex2DataIndices[sentenceIndex].add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying UMAP for projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply umap to the embeddings\n",
    "UMAPNeighbors = 15\n",
    "dimension = 2\n",
    "umapModel = umap.UMAP(n_neighbors=UMAPNeighbors, metric='cosine', n_components=dimension,n_epochs=200,verbose=True)\n",
    "umapCoordinates = umapModel.fit_transform(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting everything to a network format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ig.Graph(sentence_embeddings.shape[0],edges=edges,directed=False)\n",
    "if(weights):\n",
    "    g.es[\"weight\"] = weights\n",
    "\n",
    "g.vs[\"Label\"] = uniqueSentences\n",
    "positions = umapCoordinates*100\n",
    "# allPositions = positions\n",
    "allPositions = umapCoordinates*100\n",
    "medianPosition = np.median(allPositions,axis=0)\n",
    "\n",
    "# remove all nodes that are too far fromt he median position\n",
    "distances = np.linalg.norm(allPositions-medianPosition,axis=1)\n",
    "maxDistance = np.percentile(distances,99)\n",
    "allMainIndices = [vIndex for vIndex in range(len(allPositions)) if distances[vIndex]<maxDistance*1.1]\n",
    "mainIndices = [vIndex for vIndex in range(g.vcount()) if distances[vIndex]<maxDistance*1.1]\n",
    "g = g.subgraph(mainIndices)\n",
    "positions=positions[mainIndices,:]\n",
    "allPositions = allPositions[allMainIndices,:]\n",
    "meanPosition = np.mean(allPositions,axis=0)\n",
    "allPositions -= meanPosition\n",
    "positions -= meanPosition\n",
    "# recenter by the extremes in the PCA axes\n",
    "# first apply PCA using sklearn\n",
    "pca = PCA(n_components=dimension)\n",
    "pca.fit(allPositions)\n",
    "allPositions = pca.transform(allPositions)\n",
    "positions = pca.transform(positions)\n",
    "# recenter by the extremes in the PCA axes\n",
    "# recenter based on the extremes of the PCA axes\n",
    "minPositions = np.min(allPositions,axis=0)\n",
    "maxPositions = np.max(allPositions,axis=0)\n",
    "positions = positions - (minPositions+maxPositions)/2\n",
    "\n",
    "# swap x and y axis\n",
    "# positions = positions[:,[1,0]]\n",
    "g.vs[\"Position\"] = positions\n",
    "\n",
    "\n",
    "xn.save(g, networksPath / f\"{datasetName}_{bertModelName.replace('/','_')}_umap.xnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now download and visualize the network using Helios-web by downloading the file and dragging it to the browser at this address:\n",
    "\n",
    "http://heliosweb.io/docs/example/?advanced&layout=0&use2d&size=1&density"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ic2s2_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
