{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Network embeddings\n",
    "In this notebook you will learn how to use network embeddings to represent posts according to their textual content. We will use sentence BERT to generate embeddings for each post and then use these embeddings to train a classifier to predict the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import emlens\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "import umap\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a model. all-MiniLM-L12-v2 are already included in the dataset. You can also use any other model from the sentence-transformers library. You can find a list of models [here](https://www.sbert.net/docs/pretrained_models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName = \"debate2024_Jun_mastodon\"\n",
    "dataPath = Path(\"Data\")\n",
    "networksPath = dataPath/\"Networks\"\n",
    "networksPath.mkdir(parents=True, exist_ok=True)\n",
    "bertModelName = \"all-MiniLM-L12-v2\"\n",
    "# bertModelName = \"all-mpnet-base-v2\"\n",
    "# bertModelName = \"dmlls/all-mpnet-base-v2-negation\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(dataPath / f\"{datasetName}.feather.gz\", 'rb') as f:\n",
    "    df = pd.read_feather(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to look into the unique sentences, so that we don't generate multiple embeddings for the same sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_sentences_and_indices(sentences):\n",
    "    unique_sentences = []\n",
    "    sentence_to_index = {}\n",
    "    sentences_indices = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if sentence not in sentence_to_index:\n",
    "            sentence_to_index[sentence] = len(unique_sentences)\n",
    "            unique_sentences.append(sentence)\n",
    "        sentences_indices.append(sentence_to_index[sentence])\n",
    "    \n",
    "    return unique_sentences, sentences_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will generate and save the embeddings for all the posts in the dataset. You can skip this step if you dont have a GPU. The next cell will load the precomputed embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP THIS IF YOU ALREADY HAVE THE EMBEDDINGS\n",
    "# MAY TAKE SEVERAL MINUTES WITHOUT A GPU\n",
    "model = SentenceTransformer(bertModelName)\n",
    "uniqueSentences, sentenceIndices = get_unique_sentences_and_indices(df['text'])\n",
    "sentence_embeddings = model.encode(uniqueSentences, show_progress_bar=True)\n",
    "# save as compressed numpy\n",
    "np.savez_compressed(dataPath / f\"{datasetName}_{bertModelName.replace('/','_')}_embeddings.npz\",\n",
    "                    sentence_embeddings=sentence_embeddings,\n",
    "                    uniqueSentences=uniqueSentences,\n",
    "                    sentenceIndices=sentenceIndices,\n",
    "                    modelName = bertModelName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the embedding from existing file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to load embeddings\n",
    "with np.load(dataPath / f\"{datasetName}_{bertModelName.replace('/','_')}_embeddings.npz\") as data:\n",
    "    sentence_embeddings = data[\"sentence_embeddings\"]\n",
    "    uniqueSentences = data[\"uniqueSentences\"]\n",
    "    sentenceIndices = data[\"sentenceIndices\"]\n",
    "    modelName = str(data[\"modelName\"])\n",
    "    \n",
    "model = SentenceTransformer(modelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a helper function in case we need to encode any more sentences to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb(sentence):\n",
    "    return model.encode(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantagens of the embedding is that we can calculate the similarity between sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence0 = \"Education is fundamental for the development of a country\"\n",
    "sentence1 = \"A nations growth relies heavily on a strong educational foundation\"\n",
    "sentence2 = \"My kids are not well-educated in this nation history\"\n",
    "sentence3 = \"I'm cooking a delicious meal today\"\n",
    "\n",
    "# Compute embeddings\n",
    "embedding0 = emb(sentence0)\n",
    "embedding1 = emb(sentence1)\n",
    "embedding2 = emb(sentence2)\n",
    "embedding3 = emb(sentence3)\n",
    "\n",
    "# Compute cosine-similarities\n",
    "print(f\"Cosine-Similarity 0-1: {cosine_similarity([embedding0], [embedding1])[0][0]:.4f}\")\n",
    "print(f\"Cosine-Similarity 0-2: {cosine_similarity([embedding0], [embedding2])[0][0]:.4f}\")\n",
    "print(f\"Cosine-Similarity 0-3: {cosine_similarity([embedding0], [embedding3])[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then find the most similar sentences to a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberNeighbors = 5\n",
    "nnModel = NearestNeighbors(n_neighbors=numberNeighbors, metric='cosine').fit(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Economy is the most important issue for the country\"\n",
    "distances, indices = nnModel.kneighbors([emb(query)])\n",
    "\n",
    "for index,distance in zip(indices[0],distances[0]):\n",
    "    print(f\"Sim. {distance:.2f}\")\n",
    "    print(uniqueSentences[index])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic axes can be created in this embedding by just giving a few examples. For instance the axis for positive vs negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive vs. Negative Sentiment\n",
    "group0_sentences = [\n",
    "    \"I am hopeful about the future\",\n",
    "    \"The new policies are promising\",\n",
    "    \"The debate was enlightening\",\n",
    "    \"I support the new healthcare plan\",\n",
    "    \"The economy is showing signs of recovery\",\n",
    "    \"Education reforms are beneficial\",\n",
    "    \"The climate initiative is a step in the right direction\",\n",
    "    \"I am optimistic about the upcoming election\",\n",
    "    \"The government is handling the crisis well\",\n",
    "    \"Community efforts are making a difference\",\n",
    "]\n",
    "\n",
    "group1_sentences = [\n",
    "    \"I am worried about the economy\",\n",
    "    \"The policies are disappointing\",\n",
    "    \"The debate was a disaster\",\n",
    "    \"I oppose the new healthcare plan\",\n",
    "    \"The government is failing us\",\n",
    "    \"Education reforms are inadequate\",\n",
    "    \"The climate initiative is insufficient\",\n",
    "    \"I am pessimistic about the upcoming election\",\n",
    "    \"The crisis management is poor\",\n",
    "    \"Community efforts are not enough\",\n",
    "]\n",
    "\n",
    "# # Policy vs. Personalities\n",
    "# group0_sentences = [\n",
    "#     \"The economic plan will reduce unemployment\",\n",
    "#     \"Healthcare reforms are essential for the country\",\n",
    "#     \"Education funding needs to be increased\",\n",
    "#     \"Climate change policies must be prioritized\",\n",
    "#     \"Tax reforms will benefit the middle class\",\n",
    "#     \"Infrastructure investment is necessary\",\n",
    "#     \"Immigration policies should be humane\",\n",
    "#     \"Defense spending needs to be rationalized\",\n",
    "#     \"Social security reforms are overdue\",\n",
    "#     \"Technology regulations should be updated\",\n",
    "# ]\n",
    "\n",
    "# group1_sentences = [\n",
    "#     \"Biden is a compassionate leader\",\n",
    "#     \"Trump is a strong and decisive president\",\n",
    "#     \"Harris brings a fresh perspective\",\n",
    "#     \"Pence is a steady and reliable vice president\",\n",
    "#     \"Biden's empathy is his strength\",\n",
    "#     \"Trump's charisma is unmatched\",\n",
    "#     \"Harris is an inspiring figure\",\n",
    "#     \"Pence's loyalty is commendable\",\n",
    "#     \"Biden's experience is invaluable\",\n",
    "#     \"Trump's resilience is noteworthy\",\n",
    "# ]\n",
    "\n",
    "# # Before vs. After the Debate\n",
    "# group0_sentences = [\n",
    "#     \"I am looking forward to the debate\",\n",
    "#     \"The debate will be crucial for the election\",\n",
    "#     \"I expect strong arguments from both sides\",\n",
    "#     \"The debate will highlight key policies\",\n",
    "#     \"The debate could sway undecided voters\",\n",
    "#     \"It's important to hear both candidates\",\n",
    "#     \"The debate will provide clarity on their positions\",\n",
    "#     \"I hope the debate is civil and informative\",\n",
    "#     \"The debate will be a significant event\",\n",
    "#     \"The candidates need to perform well in the debate\",\n",
    "# ]\n",
    "\n",
    "# group1_sentences = [\n",
    "#     \"The debate was informative\",\n",
    "#     \"Biden performed well in the debate\",\n",
    "#     \"Trump's debate strategy was effective\",\n",
    "#     \"The debate did not change my opinion\",\n",
    "#     \"The candidates addressed key issues\",\n",
    "#     \"The debate highlighted their differences\",\n",
    "#     \"It was a heated debate\",\n",
    "#     \"The debate provided a clear comparison\",\n",
    "#     \"I was impressed by the candidates' knowledge\",\n",
    "#     \"The debate was a turning point in the election\",\n",
    "# ]\n",
    "\n",
    "\n",
    "group0_embeddings = emb(group0_sentences)\n",
    "group1_embeddings = emb(group1_sentences)\n",
    "\n",
    "# concatenate the embeddings\n",
    "trainData = np.concatenate([group0_embeddings, group1_embeddings], axis=0)\n",
    "labels = np.array([0]*len(group0_embeddings) + [1]*len(group1_embeddings))\n",
    "\n",
    "\n",
    "semAxisModel = emlens.SemAxis()\n",
    "semAxisModel.fit(trainData, labels)\n",
    "projectedCoordinates = semAxisModel.transform(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,3))\n",
    "bins = 30\n",
    "\n",
    "p = plt.hist(projectedCoordinates,bins=bins,density=True,alpha=0.70)\n",
    "    \n",
    "plt.setp(ax, yticks=[]);\n",
    "fig.patch.set_visible(False)\n",
    "ax.spines['top'].set_visible(True)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "ax.set_xlabel(\"SemAxis\")\n",
    "\n",
    "breaks = 15\n",
    "step = (np.max(projectedCoordinates)-np.min(projectedCoordinates))/breaks;\n",
    "addedIndices = []\n",
    "sortedIndices = sorted(range(len(projectedCoordinates)),key=lambda i: projectedCoordinates[i])\n",
    "for i in sortedIndices:\n",
    "    if(projectedCoordinates[i] >= np.min(projectedCoordinates)+len(addedIndices)*step):\n",
    "        addedIndices.append(i)\n",
    "        \n",
    "for index,senteceIndex in enumerate(addedIndices):\n",
    "#     index = 6069\n",
    "    plt.scatter([projectedCoordinates[senteceIndex]],[1.0],s=10, c = \"k\",\n",
    "                clip_on=False,\n",
    "                transform = ax.get_xaxis_transform())\n",
    "    textActor = ax.text(projectedCoordinates[senteceIndex], 1.0, str(index), fontsize=8,\n",
    "                  rotation=0, rotation_mode='anchor',\n",
    "    #               transform_rotates_text=True,\n",
    "                   transform = ax.get_xaxis_transform())\n",
    "    textActor.set_path_effects([pe.Stroke(linewidth=2, foreground='white'),\n",
    "                       pe.Normal()])\n",
    "    print(f\"{index}: {uniqueSentences[senteceIndex]}\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ic2s2_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
