{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.sparse import csr_matrix\n",
    "import igraph as ig\n",
    "import xnetwork as xn\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName = \"debate2024_Jun_bluesky\"\n",
    "dataPath = Path(\"Data\")\n",
    "networksPath = dataPath/\"Networks\"\n",
    "networksPath.mkdir(parents=True, exist_ok=True)\n",
    "# Minimum number of activities for a user to be considered\n",
    "minUserActivities = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(dataPath / f\"{datasetName}.feather.gz\", 'rb') as f:\n",
    "    df = pd.read_feather(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "userActivityCount = df[\"user_id\"].value_counts()\n",
    "usersWithMinActivities = set(userActivityCount[userActivityCount >= minUserActivities].index)\n",
    "dfFiltered = df[df[\"user_id\"].isin(usersWithMinActivities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download('en_core_web_md')\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "# Load English Stop Words\n",
    "stopword_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                           u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizePost(text, ngram_range=(1, 2)):\n",
    "\n",
    "    # Cleaning text\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', \" \", text)  # Remove URL\n",
    "    # also filter urls that do not start with https:// or http://\n",
    "    # anything that is recognized as a url\n",
    "    \n",
    "\n",
    "    text = re.sub(r'@\\w+', ' ', text)  # Remove mentions\n",
    "    text = re.sub(r'\\d+', ' ', text)  # Remove digits\n",
    "    text = re.sub(r'<.*?>', ' ', text)  # Remove HTML tags\n",
    "    text = remove_emoji(text)  # Remove emoji\n",
    "    text = re.sub(r'#\\w+', ' ', text)  # Remove hashtags\n",
    "    text = text.lstrip('RT')  # Remove RT word\n",
    "\n",
    "    # Use spaCy to tokenize and lemmatize\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.lemma_.lower() not in stopword_set and not token.is_punct and not token.is_space]\n",
    "    # if token < 4 characters, remove it\n",
    "    tokens = [token for token in tokens if len(token) > 3]\n",
    "    \n",
    "    # Include n-grams of size defined by ngram_range\n",
    "    ngrams = []\n",
    "    for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "        ngrams.extend([\" \".join(tokens[i:i+n]).lower() for i in range(len(tokens) - n + 1)])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainBipartiteEdgesWords(df,removeReposts=True,removeReplies=False, ngramSize = 1):\n",
    "    if \"text\" not in df or \"post_type\" not in df or \"user_id\" not in df:\n",
    "        return []\n",
    "    # drop all rows with missing text\n",
    "    df = df.dropna(subset=[\"text\"])\n",
    "    if(removeReposts):\n",
    "        df = df[df[\"post_type\"] != \"repost\"]\n",
    "    if(removeReplies):\n",
    "        df = df[df[\"post_type\"] != \"reply\"]\n",
    "\n",
    "    # convert url strings that looks like lists to actual lists\n",
    "    users = df[\"user_id\"]\n",
    "    textData = df[\"text\"]\n",
    "\n",
    "    tokens = df[\"text\"].progress_apply(lambda x: tokenizePost(x,ngram_range=(1,ngramSize)))\n",
    "    # keep only non-empty lists\n",
    "    mask = tokens.apply(lambda x: len(x) > 0)\n",
    "    tokens = tokens[mask]\n",
    "    users = users[mask]\n",
    "    # create edges list users -> hashtags\n",
    "    edges = [(user,token) for user,token_list in zip(users,tokens) for token in token_list]\n",
    "    return edges\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee2a4cdc2224c7f86299b6c632c3cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105747 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bipartiteEdges = obtainBipartiteEdgesWords(dfFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterNodes(bipartiteEdges, minRightDegree=1, minLeftDegree=1):\n",
    "    # goes from right to left\n",
    "    bipartiteEdges = np.array(bipartiteEdges)\n",
    "    mask = np.ones(len(bipartiteEdges),dtype=bool)\n",
    "    if(minRightDegree>1):\n",
    "        uniqueEdges = set(tuple(edge) for edge in bipartiteEdges)\n",
    "        uniqueEdges = np.array(list(uniqueEdges))\n",
    "        rightDegrees = Counter(uniqueEdges[:,1])\n",
    "        mask &= np.array([rightDegrees[rightNode]>=minRightDegree for _,rightNode in bipartiteEdges])\n",
    "    bipartiteEdges = bipartiteEdges[mask]\n",
    "    \n",
    "    # goes from left to right\n",
    "    mask = np.ones(len(bipartiteEdges),dtype=bool)\n",
    "    if(minLeftDegree>1):\n",
    "        uniqueEdges = set(tuple(edge) for edge in bipartiteEdges)\n",
    "        uniqueEdges = np.array(list(uniqueEdges))\n",
    "        leftDegrees = Counter(uniqueEdges[:,0])\n",
    "        mask &= np.array([leftDegrees[leftNode]>=minLeftDegree for leftNode,_ in bipartiteEdges])\n",
    "    bipartiteEdges = bipartiteEdges[mask]\n",
    "    return bipartiteEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bipartiteEdges = filterNodes(bipartiteEdges, minRightDegree=4, minLeftDegree=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bipartiteEdges = np.array(bipartiteEdges)\n",
    "bipartiteIndexedEdges = np.zeros(bipartiteEdges.shape, dtype=int)\n",
    "leftIndex2Label = [label for label in np.unique(bipartiteEdges[:,0])]\n",
    "leftLabel2Index = {label: index for index, label in enumerate(leftIndex2Label)}\n",
    "rightIndex2Label = [label for label in np.unique(bipartiteEdges[:,1])]\n",
    "rightLabel2Index = {label: index for index, label in enumerate(rightIndex2Label)}\n",
    "\n",
    "# create indexed edges in a numpy array integers\n",
    "bipartiteIndexedEdges[:,0] = [leftLabel2Index[label] for label in bipartiteEdges[:,0]]\n",
    "bipartiteIndexedEdges[:,1] = [rightLabel2Index[label] for label in bipartiteEdges[:,1]]\n",
    "\n",
    "leftCount = len(leftIndex2Label)\n",
    "rightCount = len(rightIndex2Label)\n",
    "\n",
    "leftIndexedDegree = np.bincount(bipartiteIndexedEdges[:,0])\n",
    "rightIndexedDegree = np.bincount(bipartiteIndexedEdges[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8271, 5528],\n",
       "       [8271, 1872],\n",
       "       [8271, 6778],\n",
       "       [8271,  172],\n",
       "       [8271, 7185]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bipartiteIndexedEdges[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsMatrix = csr_matrix((np.ones(len(bipartiteIndexedEdges)), (bipartiteIndexedEdges[:,0], bipartiteIndexedEdges[:,1])), shape=(leftCount, rightCount))\n",
    "vectorizer = TfidfTransformer()\n",
    "weightsMatrix = vectorizer.fit_transform(weightsMatrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(weightsMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a threshold to the similarities\n",
    "threshold = 0.3\n",
    "similarities[similarities<threshold] = 0\n",
    "# remove diagonal\n",
    "np.fill_diagonal(similarities, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ig.Graph.Weighted_Adjacency(similarities, mode=\"undirected\", attr=\"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Number of nodes': 8738,\n",
       " 'Number of edges': 60077,\n",
       " 'Avg. degree': 13.750743877317465}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    " \"Number of nodes\":g.vcount(),\n",
    " \"Number of edges\":g.ecount(),\n",
    " \"Avg. degree\": 2.0*g.ecount()/g.vcount()\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.vs[\"Label\"] = leftIndex2Label\n",
    "# original number of posts\n",
    "g.vs[\"URLCount\"] = leftIndexedDegree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top hashtags of each user according to tfidf\n",
    "topWords= []\n",
    "topWordsCount = 10\n",
    "for row in weightsMatrix:\n",
    "    topWords.append(\", \".join([rightIndex2Label[index] for index in row.indices[np.argsort(row.data)][:-(topWordsCount+1):-1]]))\n",
    "g.vs[\"TopWords\"] = topWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove isolated nodes\n",
    "gFiltered = g.subgraph(g.vs.select(_degree_gt=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Number of nodes': 2803,\n",
       " 'Number of edges': 60077,\n",
       " 'Avg. degree': 42.8662147698894}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "   \"Number of nodes\":gFiltered.vcount(),\n",
    "   \"Number of edges\":gFiltered.ecount(),\n",
    "   \"Avg. degree\": 2.0*gFiltered.ecount()/gFiltered.vcount()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to XNET\n",
    "xn.save(gFiltered, networksPath/f\"{datasetName}_coword.xnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ic2s2_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
